{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effbf36a-4fb2-41c7-a7fa-0101afe6e211",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "We'll explore the fundamentals of prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7f433c-6412-42c1-a567-799220e5fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2cdbd-57a7-4d29-87b3-40fcacd904d8",
   "metadata": {},
   "source": [
    "## Structure of a Prompt\n",
    "\n",
    "A prompt can consist of multiple components:\n",
    "\n",
    "* Instructions\n",
    "* External information or context\n",
    "* User input or query\n",
    "* Output indicator\n",
    "\n",
    "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
    "\n",
    "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
    "\n",
    "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
    "\n",
    "**User input or query** is typically a query directly input by the user of the system.\n",
    "\n",
    "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
    "\n",
    "Each of these components should usually be placed the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0da920-4e7c-4484-ad31-886fc84ea17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c3b1b-23a9-4794-a236-486447a2e4fa",
   "metadata": {},
   "source": [
    "In this example we have:\n",
    "\n",
    "```\n",
    "Instructions\n",
    "\n",
    "Context\n",
    "\n",
    "Question (user input)\n",
    "\n",
    "Output indicator (\"Answer: \")\n",
    "```\n",
    "\n",
    "Let's try sending this to a GPT-3 model. For this, you will need [an OpenAI API key](https://beta.openai.com/account/api-keys).\n",
    "\n",
    "We initialize a `text-davinci-003` model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16986b79-9bd3-442d-8b75-4120ca300fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "openai.api_key = \"KEY_GOES_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09068306-d8ff-45fb-9044-afa7f30fc789",
   "metadata": {},
   "source": [
    "And make a generation from our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec6779a-ed87-4917-a2ea-de0b56ca6d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "# now query text-davinci-003\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3de9b1-d15f-434d-81ee-2561c355efff",
   "metadata": {},
   "source": [
    "Alternatively, if we do have the correct information withing the `context`, the model should reply with `\"I don't know\"`, let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea21859f-3d28-403b-bc25-3aec518b9ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Libraries are places full of books.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb0d57-586d-4665-9624-86ce53555192",
   "metadata": {},
   "source": [
    "Perfect, our instructions are being understood by the model. In most real use-cases we won't be providing the external information / context to the model manually. Instead, it will be an automatic process using something like [long-term memory](https://www.pinecone.io/learn/openai-gen-qa/) to retrieve relevant information from an external source.\n",
    "\n",
    "For now, that's beyond the scope of what we're exploring here, you can find more on that in the link above.\n",
    "\n",
    "In summary, a prompt often consists of those four components: instructions, context(s), user input, and the output indicator. Now we'll take a look at creative vs. stricter generation.\n",
    "\n",
    "## Generation Temperature\n",
    "\n",
    "The `temperature` parameter used in generation models tells us how \"random\" the model can be. It represents the probability of a model to choose a word which is *not* the first choice of the model.\n",
    "\n",
    "This works because the model is actually assigning a probability prediction across all tokens within it's vocabulary with each _\"step\"_ of the model (each new word or sub-word).\n",
    "\n",
    "TK visual demonstrating steps over tokens\n",
    "\n",
    "With each new step forwards the model considers the previous tokens fed into the model, creates an embedding by encoding the information from these tokens over many model encoder layers, then passes this encoding to a decoder. The decoder then predicts the probability of each token that the model knows (ie is within the model *vocabulary*) based on the information encoded within the embedding.\n",
    "\n",
    "TK visualize the encoding at one timestep -> decoding -> prediction over many tokens\n",
    "\n",
    "At a temperature of `0.0` the decoder will always select the top predicted token. At a temperature of `1.0` the model will always select a word that *is predicted* considering it's assigned probability.\n",
    "\n",
    "TK visualize selection of always top word over several timesteps when temp == 0, compared to selection of over words over several timesteps when temp == 1\n",
    "\n",
    "Considering all of this, if we have a conservative, fact based Q&A like in the previous example, it makes sense to set a lower `temperature`. However, if we're wanting to produce some creative writing or chatbot conversations, we might want to experiment and increase `temperature`. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e23a7eaa-6a4d-4896-85cc-47291930c5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, just hanging out and having a good time. What about you?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
    "chatbot's responses are amusing and entertaining.\n",
    "\n",
    "Chatbot: Hi there! I'm a chatbot.\n",
    "User: Hi, what are you doing today?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0  # set the temperature, default is 1\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07596e8e-0d1c-43ef-a09c-cee7bb726455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here to provide assistance and answer any questions you may have.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
    "chatbot's responses are formal and serious.\n",
    "\n",
    "Chatbot: Hi there! I'm a chatbot.\n",
    "User: Hi, what are you doing today?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.0  # set the temperature, default is 1\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa19f32e-dc79-42e3-a833-9efbbcecc2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just hanging out, thinking about life and enjoying the day. How about you?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
    "chatbot's responses are amusing and entertaining.\n",
    "\n",
    "Chatbot: Hi there! I'm a chatbot.\n",
    "User: Hi, what are you doing today?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=512,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdbe9b44-2a97-49a6-9f23-4eb7b9587908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making stuff occur all over the internets. Epic debockets!!! #magazaviournederrachedirmomplamengillionwhatchinglet. What about you showing?  Always Sunday inf dayyesmateregoursounddistagoforlocologieswheeligs somewhere planoutcurrecroustarsachzerightmyguybaseingtonbergeslos-ancodeitionüêµ fingers crawickedatvincoin altexeurair galbehinesackhibarnepouchiffoofightnomonicsolecs LOL??!'#\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
    "chatbot's responses are amusing and entertaining.\n",
    "\n",
    "Chatbot: Hi there! I'm a chatbot.\n",
    "User: Hi, what are you doing today?\n",
    "Chatbot: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=512,\n",
    "    temperature=2.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754eac90-d27f-4e89-936a-fac6cf47c35a",
   "metadata": {},
   "source": [
    "## Few-shot Training\n",
    "\n",
    "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4da9bd68-6dcf-4ef3-8cef-7bf93fdb4cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42, of course!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. \n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de5cd41c-5123-4d9d-b640-4e870fac254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is to find your own meaning.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. \n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21361b2b-4631-4d8c-a31c-593d3d0e9973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42. Just kidding. The meaning of life is whatever you make of it.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following are exerpts from conversations with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd4bb15-7469-4248-af2f-aceac6ad08fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ab474-6c08-4e6d-90f6-b79ca140d400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ce940-f652-4f0e-aca7-20b31a43bdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decc7034-2d03-4d5f-bec3-04bcf930db82",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8fe8b-b74a-4e6b-8ce1-77c8c54a14a7",
   "metadata": {},
   "source": [
    "https://docs.cohere.ai/docs/prompt-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaecb5e-06fc-4241-b05d-6f8bf07377d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
