{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "effbf36a-4fb2-41c7-a7fa-0101afe6e211",
   "metadata": {},
   "source": [
    "# LangChain Playground #1\n",
    "\n",
    "At its core, LangChain is framework built around LLMs. We have use it to build chatbots, generative question-answering, summarization, and much more.\n",
    "\n",
    "The core idea of the library is that we can \"chain\" together different components to create more advanced use cases around LLMs. Chain may consist of multiple components from several modules: Prompt Templates, LLMs, Agents, and Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75f38581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f7f433c-6412-42c1-a567-799220e5fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c023a5e",
   "metadata": {},
   "source": [
    "## Using LLMs in LangChain\n",
    "LangChain support several LLM providers, like HuggingFace and OpenAI.\n",
    "Let's exmplore the different LLM integrations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ec3fb7e",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc73545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90a6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"HF_KEY_GOES_HERE\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "786257a5",
   "metadata": {},
   "source": [
    "We can then generate text using a HF Hub model (we'll use `google/flan-t5-x1`) using the Inference API built into Hugging Face Hub.\n",
    "\n",
    "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353fc54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green bay packers\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xl\",\n",
    "    model_kwargs={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "question = \"Which NFL team won the super bowl in the 2010 season?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bc6a5d1",
   "metadata": {},
   "source": [
    "Asking multiple questions we can done by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c05525d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='190', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 5 ft 10 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}  \n",
    "]\n",
    "res = llm_chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7755bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I am 6 ft 4 inches, how tall am I in centimeters\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95aa136e",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee8cb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_KEY_GOES_HERE'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f63dd55",
   "metadata": {},
   "source": [
    "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8f5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "davinci = OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03363229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Green Bay Packers won the Super Bowl in the 2010 season.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b82da6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 177.8 centimeters', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' The 12th person to walk on the moon was Harrison Schmitt, an American geologist and NASA astronaut. He was part of the Apollo 17 mission, which took place on December 11, 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 142, 'completion_tokens': 67, 'prompt_tokens': 75}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 5 ft 10 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccd96f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m qs \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWhich NFL team won the Super Bowl in the 2010 season?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIf I am 6 ft 4 inches, how tall am I in centimeters?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWho was the 12th person on the moon?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHow many eyes does a blade of grass have?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(llm_chain\u001b[39m.\u001b[39mrun(qs))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_chain' is not defined"
     ]
    }
   ],
   "source": [
    "qs = [\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
    "    \"Who was the 12th person on the moon?\",\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    "]\n",
    "print(llm_chain.run(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5991f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057342e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113be45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6ab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
