{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "effbf36a-4fb2-41c7-a7fa-0101afe6e211",
   "metadata": {},
   "source": [
    "# LangChain Playground #1\n",
    "\n",
    "At its core, LangChain is framework built around LLMs. We have use it to build chatbots, generative question-answering, summarization, and much more.\n",
    "\n",
    "The core idea of the library is that we can \"chain\" together different components to create more advanced use cases around LLMs. Chain may consist of multiple components from several modules: Prompt Templates, LLMs, Agents, and Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f38581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7f433c-6412-42c1-a567-799220e5fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c023a5e",
   "metadata": {},
   "source": [
    "## Using LLMs in LangChain\n",
    "LangChain support several LLM providers, like HuggingFace and OpenAI.\n",
    "Let's exmplore the different LLM integrations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ec3fb7e",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc73545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"KEY_GOES_HERE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "786257a5",
   "metadata": {},
   "source": [
    "We can then generate text using a HF Hub model (we'll use `google/flan-t5-x1`) using the Inference API built into Hugging Face Hub.\n",
    "\n",
    "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353fc54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green bay packers\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xl\",\n",
    "    model_kwargs={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "question = \"Which NFL team won the super bowl in the 2010 season?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bc6a5d1",
   "metadata": {},
   "source": [
    "Asking multiple questions we can done by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c05525d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='190', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 5 ft 10 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}  \n",
    "]\n",
    "res = llm_chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7755bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I am 6 ft 4 inches, how tall am I in centimeters\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95aa136e",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee8cb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"KEY_GOES_HERE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f63dd55",
   "metadata": {},
   "source": [
    "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8f5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "davinci = OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03363229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Green Bay Packers won the Super Bowl in the 2010 season.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b82da6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 177.8 cm', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' The 12th person on the moon was Charles Duke, a United States astronaut who flew on the Apollo 16 mission in April 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 128, 'completion_tokens': 53, 'prompt_tokens': 75}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 5 ft 10 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fccd96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. The Green Bay Packers\n",
      "2. 193 centimeters\n",
      "3. Alan Bean\n",
      "4. Zero.\n"
     ]
    }
   ],
   "source": [
    "qs = [\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
    "    \"Who was the 12th person on the moon?\",\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    "]\n",
    "print(llm_chain.run(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a5991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
      "If you are 6 ft 4 inches, you are 193.04 centimeters tall.\n",
      "The 12th person on the moon was Harrison Schmitt.\n",
      "A blade of grass does not have eyes.\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
