{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "effbf36a-4fb2-41c7-a7fa-0101afe6e211",
   "metadata": {},
   "source": [
    "# LangChain Playground #4.1\n",
    "## LLMs Getting Started\n",
    "\n",
    "This notebook goes over how to use the LLM class in LangChain.\n",
    "\n",
    "The LLM class is a class designed for interfacing with LLMs. There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them. In this part of the documentation, we will focus on generic LLM functionality. For details on working with a specific LLM wrapper, please see the examples in the How-To section.\n",
    "\n",
    "For this notebook, we will work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f38581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33m  DEPRECATION: lxml is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain\n",
    "%pip install -qU openai\n",
    "%pip install -qU tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a6a2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# openai API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"KEY_GOES_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16eb82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be811be",
   "metadata": {},
   "source": [
    "Generate Text: The most basic functionality an LLM has is just the ability to call it, passing in a string and getting back a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f9e119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e932eac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\n",
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5183458f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       " Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c1f4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='\\n\\nWhen I was younger\\nI thought that love\\nI was something like a fairytale\\nI would find my prince and we would be together\\n Forever\\nI was na√Øve\\nI thought that love\\nA fairytale love story\\n would come true\\nFor we would be together\\nIn every way\\nAnd I would be able to see\\nThe beauty in people', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       " Generation(text=\"\\n\\nHow could I\\nMarry someone I don't love\\n\\nHow could I\\nMarry someone who is not love\\n\\nHow could I\\nMarry someone who is all love\\n\\nHow could I\\nMarry someone who is both love and love\\n\\nHow could I\\nMarry someone who is not love\\n\\nHow could I\\nMarry someone who is both love and love\", generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "393d603d",
   "metadata": {},
   "source": [
    "You can also access provider specific information that is returned. This information is NOT standardized across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303b5b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 120,\n",
       "  'total_tokens': 4167,\n",
       "  'completion_tokens': 4047}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3152fffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*25)\n",
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc20133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 200,\n",
       "  'total_tokens': 6677,\n",
       "  'completion_tokens': 6477}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1b055",
   "metadata": {},
   "source": [
    "Number of Tokens: You can also estimate how many tokens a piece of text will be in that model. This is useful because models have a context length (and cost more for more tokens), which means you need to be aware of how long the text you are passing in is.\n",
    "\n",
    "Notice that by default the tokens are estimated using a HuggingFace tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2a0b4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(\"what a joke\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
